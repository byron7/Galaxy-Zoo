{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def networkArchitecture():\n",
    "    print (\"Arquitectura de la CNN\")\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(80, (5, 5), activation='relu', input_shape=(IMAGE_NUM_CHANNELS, IMAGE_SIZE, IMAGE_SIZE), data_format='channels_first'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "    model.add(Convolution2D(96, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Convolution2D(192, 3, 3, activation='relu'))\n",
    "\n",
    "    model.add(Convolution2D(192, 3, 3, activation='relu'))\n",
    "\n",
    "    model.add(Convolution2D(384, 3, 3, activation='relu'))\n",
    "\n",
    "    model.add(Convolution2D(384, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048))\n",
    "    model.add(Dense(37, activation='relu'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution2D()\n",
    "MaxPooling2D()\n",
    "Dense()\n",
    "activation='relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingModelo():\n",
    "    print(\"Entrenando el model ...\")\n",
    "    #obtenemos los valores guardados de las imagenes de train que provienen del pickle\n",
    "    pickle_x = open(\"./pickles/X.pickle\", \"rb\")\n",
    "    X = pickle.load(pickle_x)\n",
    "    # obtenemos los valores guardados del csv de train que provienen del pickle\n",
    "    pickle_y = open(\"./pickles/y.pickle\", \"rb\")\n",
    "    y = pickle.load(pickle_y)\n",
    "\n",
    "    #para normalizar\n",
    "    X = X / 255.0\n",
    "\n",
    "    #dividimos el dataset en entrenamiento (90%) y validacion(90%)\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    #Ajustamos el conjunto de las imagenes\n",
    "    X_train = X_train.reshape(X_train.shape[0], IMAGE_NUM_CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    X_validation = X_validation.reshape(X_validation.shape[0], IMAGE_NUM_CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "\n",
    "    model = networkArchitecture()\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, batch_size=32, epochs=30, validation_split=0.3)\n",
    "\n",
    "    #hacemos la predicciones sobre los datos de validacion\n",
    "    predicciones_val = model.predict(X_validation)\n",
    "\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"model.h5\")\n",
    "    print(\"Pesos del modelo guardados.\")\n",
    "\n",
    "    print(\"Finalizacion de entrenamiento.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss='mean_squared_error'\n",
    "optimizer='adam'\n",
    "metrics=['accuracy']\n",
    "batch_size=32\n",
    "epochs=30\n",
    "validation_split=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta funci√≥n es utilizada para cargar el modelo previamente entrenado.\n",
    "\n",
    "def loadModeltesting():\n",
    "    json_file = open('model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "    loaded_model.load_weights(\"model.h5\")\n",
    "    print(\"Modelo cargado desde el archivo\")\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingModelo():\n",
    "\n",
    "    #obtenemos la ruta donde se encuntran el csv de las imagenes de test\n",
    "    test_data_path = './csv/testGalaxyZoo.csv'\n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "    # obtenemos los valores guardados de las imagenes de test que provienen del pickle\n",
    "    pickle_x_test = open(\"./pickles/X_test.pickle\", \"rb\")\n",
    "    X_t = pickle.load(pickle_x_test)\n",
    "\n",
    "    # Normalizar\n",
    "    X_t = X_t / 255.0\n",
    "\n",
    "    X_test = X_t.reshape(X_t.shape[0], IMAGE_NUM_CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    print(\"Inicializa el testing ...\")\n",
    "    #Predecimos los valores de las imagenes de test\n",
    "    if path.exists('model.json'):\n",
    "        model=loadModeltesting()\n",
    "    else:\n",
    "        model = trainingModelo()\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    predicciones_test = model.predict(X_test)\n",
    "\n",
    "    #print(predicciones_test)\n",
    "\n",
    "    #Guardamos y generamos el csv\n",
    "    output = pd.DataFrame(data=np.array(predicciones_test), columns=clases)\n",
    "    output.insert(loc=0, column='GalaxyID', value=test_data.GalaxyID)\n",
    "    output.to_csv('csv/galaxyPredictions.csv', index=False)\n",
    "\n",
    "    print(\"Finalizacion del testing.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
